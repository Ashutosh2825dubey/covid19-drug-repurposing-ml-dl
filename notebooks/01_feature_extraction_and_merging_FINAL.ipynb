{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc1fbde5",
   "metadata": {},
   "source": [
    "# StepÂ 1: Comprehensive Feature Extraction & Dataset Assembly\n",
    "\n",
    "This notebook consolidates **all featureâ€‘engineering steps** used in the project:\n",
    "\n",
    "1. Physicochemical Properties (**PCP**)\n",
    "2. Aminoâ€‘Acid Composition (**AAC**)\n",
    "3. Network Centrality Metrics (from **Cytoscape**)\n",
    "4. Geneâ€‘Ontology (**GO**) oneâ€‘hot features (61 curated terms)\n",
    "\n",
    "At the end, we **merge** every feature block into a single modelâ€‘ready CSV.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67da23e",
   "metadata": {},
   "source": [
    "## Imports & Common Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c7c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os, subprocess, json, itertools, numpy as np\n",
    "\n",
    "# Adjust relative paths as needed for your repo structure\n",
    "RAW_DIR = \"./data/raw/\"            # FASTA sequences\n",
    "FEATURE_DIR = \"./data/features/\"    # Where feature blocks will be written\n",
    "MERGED_OUT = \"./data/processed/merged_dataset.csv\"\n",
    "\n",
    "os.makedirs(FEATURE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1ae2b",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ PCPÂ &Â AAC Extraction viaÂ *pfeature*\n",
    "\n",
    "**Approach used in this project**  \n",
    "- Leverage the standalone *pfeature* script (`pfeature_comp.py`) to batchâ€‘process FASTA files.  \n",
    "- For â‰ˆ20â€¯k negative sequences, an **automated loop** was necessary; running each sequence interactively was infeasible.\n",
    "\n",
    "Below is a *single* function that can be pointed at any FASTA file or folder and will output a CSV of either PCP or AAC features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64495388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pfeature_batch(fasta_path: str, out_csv: str, job: str = \"PCP\"):\n",
    "    \"\"\"Run pfeature on a FASTA file and dump selected feature block.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fasta_path : str\n",
    "        Path to input FASTA.\n",
    "    out_csv : str\n",
    "        Path to write CSV features.\n",
    "    job : str\n",
    "        'PCP' or 'AAC'.\n",
    "    \"\"\"\n",
    "    script = \"pfeature_comp.py\"  # assuming it's on your PATH; change if necessary\n",
    "    cmd = f\"python {script} -i {fasta_path} -o {out_csv} -j {job}\"\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "# Example (commented):\n",
    "# run_pfeature_batch(\"./data/raw/positive_data_sample.fasta\", f\"{FEATURE_DIR}/pcp_pos.csv\", \"PCP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87595a",
   "metadata": {},
   "source": [
    "> **Note**â€ƒThe full dataset extraction was executed offline prior to publishing to keep the repo lightweight.  \n",
    "Reâ€‘run the function above locally if you need to regenerate `pcp_*.csv` and `aac_*.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641b5b1",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Network Centrality Metrics (Cytoscape)\n",
    "\n",
    "1. Export *UniProt* interaction tables (with `Interacts with` column) for positive & negative sets.  \n",
    "2. Import into **Cytoscape** â†’ `Tools â–¸ NetworkAnalyzer â–¸ Analyze Network` (undirected).  \n",
    "3. *NetworkAnalyzer* outputs a node table; export as TSV â†’ drop it in `./data/features/centrality.tsv`.\n",
    "\n",
    "The cell below simply loads that export and does minimal cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_path = f\"{FEATURE_DIR}/centrality.tsv\"  # ensure file placed manually\n",
    "if os.path.exists(centrality_path):\n",
    "    centrality_df = pd.read_csv(centrality_path, sep='\\t')\n",
    "    # keep a subset of informative columns\n",
    "    keep_cols = [c for c in centrality_df.columns if c.lower() in {\n",
    "        'degree', 'betweennesscentrality', 'closenesscentrality',\n",
    "        'averageshortestpathlength', 'stress'\n",
    "    }]\n",
    "    centrality_df = centrality_df[['name'] + keep_cols]\n",
    "else:\n",
    "    centrality_df = pd.DataFrame()\n",
    "centrality_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff532dd",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Gene Ontology (GO) Feature Engineering\n",
    "\n",
    "###  Objective:\n",
    "To identify GO terms **consistently enriched** in COVID-associated proteins compared to non-COVID (human) proteins,\n",
    "and generate binary (one-hot) GO feature vectors for machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "###  Process Summary:\n",
    "\n",
    "1. **Input Files**:\n",
    "   - `ppsitive_raw.csv`: COVID-positive proteins with GO annotations.\n",
    "   - `uniprotkb_human_AND_reviewed_true_AND_m_2025_06_06.xlsx`: reviewed human proteins as negative examples.\n",
    "\n",
    "2. **GO Parsing**:\n",
    "   - Extracted `Gene Ontology IDs` from both datasets.\n",
    "   - Built a **binary matrix**: 1 if protein has GO term, else 0.\n",
    "\n",
    "3. **Statistical Testing**:\n",
    "   - Ran **50 bootstraps**, each sampling negative proteins equal in number to positive ones.\n",
    "   - For each GO term, applied **Fisherâ€™s Exact Test**:\n",
    "     > Is the term more common in COVID proteins than non-COVID?\n",
    "\n",
    "4. **Multiple Testing Correction**:\n",
    "   - Adjusted p-values using **FDR (Benjaminiâ€“Hochberg)**.\n",
    "   - Counted how often each term was significant across all bootstraps.\n",
    "\n",
    "5. **Thresholding**:\n",
    "   - Selected GO terms significant in â‰¥70% of bootstraps.\n",
    "\n",
    "6. **Output Files**:\n",
    "   - `robust_covid_enriched_go_terms.csv`: GO terms enriched across bootstraps\n",
    "   - `covid_go_features_filtered.csv`: One-hot GO matrix (COVID)\n",
    "   - `noncovid_go_features_filtered.csv`: One-hot GO matrix (non-COVID)\n",
    "\n",
    "These filtered GO features were then merged with PCP, AAC, and centrality features to build the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de9a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from collections import defaultdict\n",
    "\n",
    "# === File paths ===\n",
    "covid_file = './data/raw/ppsitive_raw.csv'\n",
    "noncovid_file = './data/raw/uniprotkb_human_AND_reviewed_true_AND_m_2025_06_06.xlsx'\n",
    "\n",
    "bootstrap_iters = 50\n",
    "alpha = 0.05\n",
    "min_consistency = 0.7\n",
    "\n",
    "# === Load data ===\n",
    "covid_df = pd.read_csv(covid_file)\n",
    "noncovid_df = pd.read_excel(noncovid_file)\n",
    "\n",
    "covid_df['GO_list'] = covid_df['Gene Ontology IDs'].fillna('').apply(lambda x: x.split(';') if x else [])\n",
    "noncovid_df['GO_list'] = noncovid_df['Gene Ontology IDs'].fillna('').apply(lambda x: x.split(';') if x else [])\n",
    "\n",
    "covid_size = covid_df.shape[0]\n",
    "all_go_terms = set(term for row in pd.concat([covid_df['GO_list'], noncovid_df['GO_list']]) for term in row)\n",
    "all_go_terms = sorted(all_go_terms)\n",
    "\n",
    "def create_binary_matrix(df, go_terms):\n",
    "    return pd.DataFrame(\n",
    "        [[1 if go in terms else 0 for go in go_terms] for terms in df['GO_list']],\n",
    "        columns=go_terms,\n",
    "        index=df['Entry']\n",
    "    )\n",
    "\n",
    "covid_matrix = create_binary_matrix(covid_df, all_go_terms)\n",
    "go_significance_counts = defaultdict(int)\n",
    "\n",
    "for i in range(bootstrap_iters):\n",
    "    noncovid_sample = noncovid_df.sample(n=covid_size, random_state=i).reset_index(drop=True)\n",
    "    noncovid_matrix = create_binary_matrix(noncovid_sample, all_go_terms)\n",
    "\n",
    "    p_values = []\n",
    "    go_terms_tested = []\n",
    "\n",
    "    for go in all_go_terms:\n",
    "        a = covid_matrix[go].sum()\n",
    "        c = noncovid_matrix[go].sum()\n",
    "        b = covid_size - a\n",
    "        d = covid_size - c\n",
    "        if a + c == 0:\n",
    "            continue\n",
    "        _, p = fisher_exact([[a, b], [c, d]], alternative='greater')\n",
    "        p_values.append(p)\n",
    "        go_terms_tested.append(go)\n",
    "\n",
    "    reject, _, _, _ = multipletests(p_values, alpha=alpha, method='fdr_bh')\n",
    "    for go_term, is_sig in zip(go_terms_tested, reject):\n",
    "        if is_sig:\n",
    "            go_significance_counts[go_term] += 1\n",
    "\n",
    "go_frac_significant = {go: count / bootstrap_iters for go, count in go_significance_counts.items()}\n",
    "robust_go_terms = [go for go, frac in go_frac_significant.items() if frac >= min_consistency]\n",
    "\n",
    "covid_filtered_matrix = covid_matrix[robust_go_terms]\n",
    "noncovid_full_matrix = create_binary_matrix(noncovid_df, all_go_terms)[robust_go_terms]\n",
    "\n",
    "covid_filtered_matrix.to_csv('./data/features/covid_go_features_filtered.csv')\n",
    "noncovid_full_matrix.to_csv('./data/features/noncovid_go_features_filtered.csv')\n",
    "pd.DataFrame(robust_go_terms, columns=['GO_term']).to_csv('./data/features/robust_covid_enriched_go_terms.csv', index=False)\n",
    "\n",
    "print(\"Saved filtered GO feature matrices and robust term list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7833d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "covid_df = pd.read_csv('./data/raw/ppsitive_raw.csv')\n",
    "noncovid_df = pd.read_excel('./data/raw/uniprotkb_human_AND_reviewed_true_AND_m_2025_06_06.xlsx')\n",
    "\n",
    "# Load curated GO terms\n",
    "curated_go = pd.read_csv('./data/features/robust_covid_enriched_go_terms.csv')['GO_term'].tolist()\n",
    "\n",
    "# Clean and split GO annotations\n",
    "covid_df['GO_list'] = covid_df['Gene Ontology IDs'].fillna('').apply(lambda x: x.split(';') if x else [])\n",
    "noncovid_df['GO_list'] = noncovid_df['Gene Ontology IDs'].fillna('').apply(lambda x: x.split(';') if x else [])\n",
    "\n",
    "def make_one_hot(df, curated_terms):\n",
    "    one_hot = pd.DataFrame(index=df['Entry'])\n",
    "    for go in curated_terms:\n",
    "        one_hot[go] = df['GO_list'].apply(lambda x: 1 if go in x else 0)\n",
    "    return one_hot.reset_index()\n",
    "\n",
    "# Create feature matrices\n",
    "covid_go_features = make_one_hot(covid_df, curated_go)\n",
    "noncovid_go_features = make_one_hot(noncovid_df, curated_go)\n",
    "\n",
    "# Save\n",
    "covid_go_features.to_csv('./data/features/covid_go_features_filtered.csv', index=False)\n",
    "noncovid_go_features.to_csv('./data/features/noncovid_go_features_filtered.csv', index=False)\n",
    "\n",
    "print(\"One-hot GO feature matrices saved:\")\n",
    "print(\"- covid_go_features_filtered.csv\")\n",
    "print(\"- noncovid_go_features_filtered.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f38dd",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Merge Feature Blocks into OneÂ DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c70387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Paths\n",
    "# ------------------------------------------------------------------\n",
    "FEATURE_DIR = Path(\"./data/features\")\n",
    "OUTPUT_FILE = Path(\"./data/processed/final_feature_matrix.csv\")\n",
    "OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Helper: read + rename the key column to a common name (â€œproteinâ€)\n",
    "# ------------------------------------------------------------------\n",
    "def load_features(path, key_cols=(\"Entry\", \"name\", \"protein\")):\n",
    "    \"\"\"Load a feature CSV/TSV and rename its key column to 'protein'.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    for col in key_cols:\n",
    "        if col in df.columns:\n",
    "            df = df.rename(columns={col: \"protein\"})\n",
    "            break\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Load each feature block\n",
    "#    â€“ adjust filenames if yours differ\n",
    "# ------------------------------------------------------------------\n",
    "pcp_df        = load_features(FEATURE_DIR / \"pcp_features.csv\")\n",
    "aac_df        = load_features(FEATURE_DIR / \"aac_features.csv\")\n",
    "centrality_df = load_features(FEATURE_DIR / \"centrality_features.csv\")\n",
    "go_df         = load_features(FEATURE_DIR / \"covid_go_features_filtered.csv\")\n",
    "#  â¬†ï¸  swap the GO file for the nonâ€‘COVID matrix when merging that set\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Guard against duplicate NONâ€‘key columns before merging\n",
    "# ------------------------------------------------------------------\n",
    "def drop_duplicate_cols(base, new):\n",
    "    dupes = (set(base.columns) & set(new.columns)) - {\"protein\"}\n",
    "    return new.drop(columns=list(dupes))\n",
    "\n",
    "aac_df        = drop_duplicate_cols(pcp_df, aac_df)\n",
    "centrality_df = drop_duplicate_cols(pd.concat([pcp_df, aac_df], axis=1), centrality_df)\n",
    "go_df         = drop_duplicate_cols(pd.concat([pcp_df, aac_df, centrality_df], axis=1), go_df)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Sequential merge\n",
    "#    - inner for core (PCP âˆ© AAC) to keep only proteins in both sets\n",
    "#    - left joins afterwards so we don't lose proteins that are\n",
    "#      missing centrality or GO annotations\n",
    "# ------------------------------------------------------------------\n",
    "merged = pcp_df.merge(aac_df,        on=\"protein\", how=\"inner\")\n",
    "merged = merged.merge(centrality_df, on=\"protein\", how=\"left\")\n",
    "merged = merged.merge(go_df,         on=\"protein\", how=\"left\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. Save and quick sanity check\n",
    "# ------------------------------------------------------------------\n",
    "merged.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"âœ…  Final feature matrix written to: {OUTPUT_FILE}  ({merged.shape[0]} proteins Ã— {merged.shape[1]-1} features)\")\n",
    "merged.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db003eb",
   "metadata": {},
   "source": [
    "## ðŸŽ‰Â Done\n",
    "\n",
    "- **PCP & AAC** extracted via automated *pfeature* batch script.\n",
    "- **Centrality metrics** computed in Cytoscape & imported.\n",
    "- **GO oneâ€‘hot features** constructed for 61 curated terms.\n",
    "- All blocks merged â†’ `data/processed/merged_dataset.csv` (modelâ€‘ready).\n",
    "\n",
    "Proceed to **`02_feature_selection.ipynb`** for downstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2fada",
   "metadata": {},
   "source": [
    "## 5 Balanced Dataset Generation (47 Sets)\n",
    "\n",
    "To address class imbalance between positive (SARS-CoV-2) and negative (human) proteins, we generate **47 balanced datasets** by:\n",
    "\n",
    "- Fixing the 335 positive proteins\n",
    "- Randomly sampling 335 negatives from a pool of ~15,000 reviewed human proteins\n",
    "- Merging both sets and shuffling\n",
    "\n",
    "Each dataset (`dataset1.csv`, ..., `dataset44.csv`) contains:\n",
    "- 335 positive samples (label = 1)\n",
    "- 335 negative samples (label = 0)\n",
    "\n",
    "These balanced datasets are used for training 44 separate models for robust evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df084ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load positive and negative sets\n",
    "positive = pd.read_csv('./data/processed/positive_final_feature_matrix.csv')\n",
    "negative = pd.read_csv('./data/processed/negative_final_feature_matrix.csv')\n",
    "\n",
    "# Check shape\n",
    "assert positive.shape[0] == 335, \"Positive dataset must have 335 proteins\"\n",
    "assert negative.shape[0] >= 335, \"Not enough negatives to sample from\"\n",
    "\n",
    "# Add labels\n",
    "positive['label'] = 1\n",
    "negative['label'] = 0\n",
    "\n",
    "# Output directory\n",
    "output_dir = './data/processed/balanced_datasets/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate 47 balanced datasets\n",
    "for i in range(1, 48):  # 1 to 47\n",
    "    sampled_neg = negative.sample(n=335, random_state=i)\n",
    "    balanced = pd.concat([positive, sampled_neg]).sample(frac=1, random_state=i).reset_index(drop=True)\n",
    "    balanced.to_csv(f'{output_dir}/dataset{i}.csv', index=False)\n",
    "    print(f\"âœ… Created dataset{i}.csv with {balanced.shape[0]} samples\")\n",
    "\n",
    "print(\"âœ… All 47 balanced datasets created in:\", output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
