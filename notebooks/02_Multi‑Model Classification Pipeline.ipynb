{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi‑Model Classification Pipeline\n",
    "\n",
    "COVID‑19 Protein Dataset (44 balanced CSVs)\n",
    "\n",
    "This notebook performs **everything in one place**:\n",
    "\n",
    "1. EDA preview (quick glance at a sample dataset)  \n",
    "2. Dataset cleaning & feature engineering (based on EDA)  \n",
    "3. Full training & evaluation loop for **six classifiers**  \n",
    "4. Summary table + optional plots\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Classifiers Evaluated\n",
    "| Alias | Model | GridSearch Parameters |\n",
    "|-------|-------|-----------------------|\n",
    "| logreg | LogisticRegression | `C = [0.1, 1, 10]` |\n",
    "| svm    | SVC (RBF + linear) | `C = [1, 10]`, `gamma = ['scale','auto']` |\n",
    "| knn    | KNeighbors          | `n_neighbors = [3, 5, 7]`, `weights = ['uniform','distance']` |\n",
    "| dtree  | DecisionTree        | `max_depth = [5, 10, 15]`, `min_samples_split = [2, 5]` |\n",
    "| rf     | RandomForest        | see code (depth, leaves, samples, etc.) |\n",
    "| gb     | GradientBoosting    | `n_estimators = [100, 200]`, `learning_rate = [0.05, 0.1]` |\n",
    "\n",
    "---\n",
    "\n",
    "**Input:** every CSV in `./data/processed/balanced_datasets`  \n",
    "(335 positive + 335 negative proteins each)  \n",
    "\n",
    "**Output:** `all_model_results.csv` + accuracy/F1 plots per model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2️⃣ Imports & Folder Setup (+ optional EDA peek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 📂 Dataset folder (47 balanced CSVs)\n",
    "default_path = \"./data/processed/balanced_datasets\"\n",
    "folder_path = input(f\"📂 Path to dataset folder (default: {default_path}): \").strip() or default_path\n",
    "all_files   = sorted([f for f in os.listdir(folder_path) if f.endswith(\".csv\")])\n",
    "\n",
    "# 🔍 Optional quick EDA preview\n",
    "sample_df = pd.read_csv(os.path.join(folder_path, all_files[0]))\n",
    "print(\"Sample dataset shape:\", sample_df.shape)\n",
    "print(sample_df.dtypes.value_counts())\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.heatmap(sample_df.select_dtypes(include=[np.number]).iloc[:,:20].corr(),\n",
    "            cmap='coolwarm', cbar=False)\n",
    "plt.title(\"Correlation preview (first 20 numeric features)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3️⃣ Cleaning function & Model definitions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Columns deemed irrelevant after EDA\n",
    "DROP_COLS = ['Gene ID', 'NumberOfDirectedEdges', 'selected', 'IsSingleNode', 'Radiality']\n",
    "\n",
    "def clean_df(df):\n",
    "    \"\"\"EDA‑driven cleaning & feature filtering.\"\"\"\n",
    "    df = df.drop(columns=DROP_COLS, errors='ignore')\n",
    "    df.replace([np.inf, -np.inf], 1, inplace=True)\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    df = df.select_dtypes(include=[np.number])          # keep numeric only\n",
    "\n",
    "    # High‑correlation filter |r| > 0.9\n",
    "    corr = df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones_like(corr), k=1).astype(bool))\n",
    "    df = df.drop(columns=[c for c in upper.columns if any(upper[c] > 0.9)])\n",
    "    return df\n",
    "\n",
    "# Model dictionary\n",
    "MODELS = {\n",
    "    \"Logistic Regression\": (\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        {'C':[0.1,1,10]}\n",
    "    ),\n",
    "    \"SVM\": (\n",
    "        SVC(probability=True),\n",
    "        {'C':[1,10], 'gamma':['scale','auto']}\n",
    "    ),\n",
    "    \"KNN\": (\n",
    "        KNeighborsClassifier(),\n",
    "        {'n_neighbors':[3,5,7], 'weights':['uniform','distance']}\n",
    "    ),\n",
    "    \"Decision Tree\": (\n",
    "        DecisionTreeClassifier(),\n",
    "        {'max_depth':[5,10,15], 'min_samples_split':[2,5]}\n",
    "    ),\n",
    "    \"Random Forest\": (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {'n_estimators':[200,300], 'max_depth':[8,10],\n",
    "         'min_samples_split':[20,30], 'min_samples_leaf':[15,25],\n",
    "         'max_samples':[0.7,0.8], 'max_features':['sqrt']}\n",
    "    ),\n",
    "    \"Gradient Boosting\": (\n",
    "        GradientBoostingClassifier(),\n",
    "        {'n_estimators':[100,200], 'learning_rate':[0.05,0.1], 'max_depth':[3,5]}\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4️⃣  Main loop (over 47 datasets & 6 models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results = []\n",
    "\n",
    "for file in all_files:\n",
    "    print(f\"\\n📂 Processing {file}\")\n",
    "    df_raw = pd.read_csv(os.path.join(folder_path, file))\n",
    "    df = clean_df(df_raw)\n",
    "\n",
    "    if 'result_x' not in df.columns:\n",
    "        print(\"❌ No target column 'result_x' — skipped\"); continue\n",
    "\n",
    "    X, y = df.drop(columns=['result_x']), df['result_x']\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train_sc, X_test_sc = scaler.transform(X_train), scaler.transform(X_test)\n",
    "\n",
    "    # PCA component sweep\n",
    "    step = 10 if n_features >= 10 else 1\n",
    "    pc_list = list(range(step, n_features+1, step))\n",
    "    if n_features not in pc_list: pc_list.append(n_features)\n",
    "\n",
    "    for model_name, (base_model, grid) in MODELS.items():\n",
    "        print(f\"   🔍 {model_name}\")\n",
    "        best_cv, best_est, best_pca, best_n = 0, None, None, None\n",
    "\n",
    "        for n in pc_list:\n",
    "            pca = PCA(n_components=n).fit(X_train_sc)\n",
    "            Xtr_pca = pca.transform(X_train_sc)\n",
    "\n",
    "            grid_cv = GridSearchCV(base_model, grid, cv=10,\n",
    "                                   scoring='accuracy', n_jobs=-1)\n",
    "            grid_cv.fit(Xtr_pca, y_train)\n",
    "\n",
    "            if grid_cv.best_score_ > best_cv:\n",
    "                best_cv, best_est = grid_cv.best_score_, grid_cv.best_estimator_\n",
    "                best_pca, best_n  = pca, n\n",
    "\n",
    "        # Test evaluation\n",
    "        y_pred = best_est.predict(best_pca.transform(X_test_sc))\n",
    "\n",
    "        overall_results.append({\n",
    "            \"Dataset\"       : file,\n",
    "            \"Model\"         : model_name,\n",
    "            \"PCA_Components\": best_n,\n",
    "            \"Accuracy\"      : accuracy_score(y_test, y_pred),\n",
    "            \"Precision\"     : precision_score(y_test, y_pred, zero_division=0),\n",
    "            \"Recall\"        : recall_score(y_test, y_pred, zero_division=0),\n",
    "            \"F1-Score\"      : f1_score(y_test, y_pred, zero_division=0),\n",
    "            \"CV_Mean\"       : best_cv\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5️⃣ Results summary & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(overall_results).sort_values(by='F1-Score', ascending=False)\n",
    "display(summary_df.head(20))\n",
    "\n",
    "out_csv = os.path.join(folder_path, \"all_model_results.csv\")\n",
    "summary_df.to_csv(out_csv, index=False)\n",
    "print(f\"\\n✅ Full results saved → {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Deep‑Learning Evaluation (Fully Connected Network)\n",
    "\n",
    "In addition to classical ML models, we train a simple feed‑forward neural network\n",
    "on **each of the 44 datasets**.  \n",
    "For every dataset we:\n",
    "\n",
    "1. Clean & scale features (same logic as before)  \n",
    "2. Sweep PCA components (10 → 128)  \n",
    "3. Tune a small network (neurons = 64 or 128) with 5‑fold Stratified CV  \n",
    "4. Evaluate on a held‑out test split  \n",
    "5. Save a combined PDF report (`dl_all_datasets_report.pdf`) with:  \n",
    "   * Train/Test accuracy  \n",
    "   * Best hyper‑parameters  \n",
    "   * Classification report  \n",
    "   * Confusion‑matrix image  \n",
    "6. Identify & print the best‑performing dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep‑Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, traceback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from fpdf import FPDF\n",
    "\n",
    "neurons_list   = [64, 128]\n",
    "batch_sizes    = [32]\n",
    "epochs_list    = [50]\n",
    "early_stop     = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "dl_summaries        = []\n",
    "best_dataset_score  = 0\n",
    "best_dataset_record = None\n",
    "\n",
    "pdf = FPDF()\n",
    "\n",
    "for file in all_files:\n",
    "    try:\n",
    "        print(f\"\\n🧠 DL Processing {file}\")\n",
    "        df_raw = pd.read_csv(os.path.join(folder_path, file))\n",
    "        df = clean_df(df_raw.copy())\n",
    "\n",
    "        if 'result_x' not in df.columns:\n",
    "            print(\"❌ Skipping (missing target column)\"); continue\n",
    "\n",
    "        X, y = df.drop(columns=['result_x']), df['result_x']\n",
    "        if X.shape[1] < 10:\n",
    "            print(\"❌ Skipping (too few features for PCA)\"); continue\n",
    "\n",
    "        scaler = MinMaxScaler().fit(X)\n",
    "        X_scaled = scaler.transform(X)\n",
    "\n",
    "        max_components = min(128, X.shape[1])\n",
    "        pca_space = list(range(10, max_components+1, 10))\n",
    "        if max_components not in pca_space:\n",
    "            pca_space.append(max_components)\n",
    "\n",
    "        best_cv, best_model, best_pca, best_cfg = 0, None, None, None\n",
    "\n",
    "        for n_comp in pca_space:\n",
    "            pca = PCA(n_components=n_comp).fit(X_scaled)\n",
    "            X_pca = pca.transform(X_scaled)\n",
    "\n",
    "            for neurons in neurons_list:\n",
    "                for batch in batch_sizes:\n",
    "                    for n_epochs in epochs_list:\n",
    "                        cv_scores = []\n",
    "                        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "                        for tr_idx, val_idx in skf.split(X_pca, y):\n",
    "                            X_tr, X_val = X_pca[tr_idx], X_pca[val_idx]\n",
    "                            y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "\n",
    "                            model = Sequential([\n",
    "                                Dense(neurons, activation='relu', input_shape=(n_comp,), kernel_regularizer=l2(0.001)),\n",
    "                                Dropout(0.3),\n",
    "                                Dense(neurons//2, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "                                Dropout(0.3),\n",
    "                                Dense(1, activation='sigmoid')\n",
    "                            ])\n",
    "                            model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                            model.fit(X_tr, y_tr, validation_data=(X_val, y_val),\n",
    "                                      epochs=n_epochs, batch_size=batch,\n",
    "                                      verbose=0, callbacks=[early_stop])\n",
    "\n",
    "                            val_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "                            cv_scores.append(accuracy_score(y_val, val_pred))\n",
    "\n",
    "                        mean_cv = np.mean(cv_scores)\n",
    "                        if mean_cv > best_cv:\n",
    "                            best_cv   = mean_cv\n",
    "                            best_model= model\n",
    "                            best_pca  = pca\n",
    "                            best_cfg  = {\"PCA\": n_comp, \"neurons\": neurons, \"batch\": batch, \"epochs\": n_epochs}\n",
    "\n",
    "        # Train/test split\n",
    "        X_tr_raw, X_ts_raw, y_tr, y_ts = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "        X_tr = best_pca.transform(scaler.transform(X_tr_raw))\n",
    "        X_ts = best_pca.transform(scaler.transform(X_ts_raw))\n",
    "\n",
    "        y_tr_pred = (best_model.predict(X_tr) > 0.5).astype(int)\n",
    "        y_ts_pred = (best_model.predict(X_ts) > 0.5).astype(int)\n",
    "\n",
    "        train_acc = accuracy_score(y_tr, y_tr_pred)\n",
    "        test_acc = accuracy_score(y_ts, y_ts_pred)\n",
    "        cls_rep  = classification_report(y_ts, y_ts_pred)\n",
    "        conf_mat = confusion_matrix(y_ts, y_ts_pred)\n",
    "\n",
    "        dl_summaries.append({\n",
    "            \"dataset\"  : file,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\" : test_acc,\n",
    "            \"config\"   : best_cfg,\n",
    "            \"report\"   : cls_rep,\n",
    "            \"conf_mat\" : conf_mat\n",
    "        })\n",
    "\n",
    "        if test_acc > best_dataset_score:\n",
    "            best_dataset_score  = test_acc\n",
    "            best_dataset_record = dl_summaries[-1]\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        fig, ax = plt.subplots(figsize=(3,3))\n",
    "        ax.matshow(conf_mat, cmap=\"Blues\", alpha=0.8)\n",
    "        for i in range(conf_mat.shape[0]):\n",
    "            for j in range(conf_mat.shape[1]):\n",
    "                ax.text(j, i, conf_mat[i,j], va='center', ha='center')\n",
    "        plt.title(f\"{file} Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        img_path = os.path.join(tempfile.gettempdir(), f\"{file}_cm.png\")\n",
    "        plt.savefig(img_path); plt.close()\n",
    "\n",
    "        # Save to PDF\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", 'B', 12)\n",
    "        pdf.cell(0, 8, f\"Dataset: {file}\", ln=True)\n",
    "        pdf.set_font(\"Arial\", size=11)\n",
    "        pdf.cell(0, 8, f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\", ln=True)\n",
    "        pdf.cell(0, 8, f\"Best Config: {best_cfg}\", ln=True)\n",
    "        pdf.ln(4)\n",
    "        pdf.set_font(\"Courier\", size=8)\n",
    "        for line in cls_rep.splitlines():\n",
    "            pdf.cell(0, 4, line.strip(), ln=True)\n",
    "        pdf.image(img_path, x=10, w=80)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error:\", file, e)\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "# Save full PDF\n",
    "pdf_path = \"dl_all_datasets_report.pdf\"\n",
    "pdf.output(pdf_path)\n",
    "print(f\"\\n✅ Combined DL report saved → {pdf_path}\")\n",
    "\n",
    "if best_dataset_record:\n",
    "    print(\"\\n🏆 Best Performing Dataset:\")\n",
    "    print(\"Dataset:\", best_dataset_record[\"dataset\"])\n",
    "    print(\"Test Accuracy:\", round(best_dataset_record[\"test_acc\"], 4))\n",
    "    print(\"Config:\", best_dataset_record[\"config\"])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
